{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n!ls /kaggle/input/\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/guchio-quest-e002","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/google-quest-e030-head-tail","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\ndef OSprint(string):\n    os.system(f'echo \\\"{string}\\\"')\n    print(string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 5\nDEVICE = 'cuda'\n# DEVICE = 'cpu'\nBATCH_SIZE = 10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/sacremoses-master/sacremoses > /dev/null\n#!pip install /kaggle/input/transformers/transformers-master #> /dev/null\n!pip install --no-deps /kaggle/input/guchio-transformers/*.whl #> /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import BertModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertModel.from_pretrained('/kaggle/input/guchio-quest-e002')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nbest_dict = torch.load('/kaggle/input/google-quest-e030-head-tail/best_dict.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom math import ceil, floor\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\n\nfrom transformers import BertTokenizer\n\n\ndef seed_everything(seed=71):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything()\n\n\n# --- dataset ---\nclass QUESTDataset(Dataset):\n    def __init__(self, df, mode, tokens, augment,\n                 pretrained_model_name_or_path, TBSEP='[TBSEP]',\n                 MAX_SEQUENCE_LENGTH=None, use_category=True, logger=None):\n        self.mode = mode\n        self.augment = augment\n        self.len = len(df)\n        self.TBSEP = TBSEP\n        if MAX_SEQUENCE_LENGTH:\n            self.MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH\n        else:\n            raise NotImplementedError\n            # self.MAX_SEQUENCE_LENGTH = -1\n            # for i, row in self.prep_df.iterrows():\n            #     input_ids = row['input_ids'].squeeze()\n            #     if self.MAX_SEQUENCE_LENGTH < len(input_ids):\n            #         self.MAX_SEQUENCE_LENGTH = len(input_ids)\n            # sel_log(f'calculated seq_len: {self.MAX_SEQUENCE_LENGTH}', logger)\n        self.use_category = use_category\n        self.logger = logger\n        self.cat_dict = {\n            'CAT_TECHNOLOGY'.casefold(): 0,\n            'CAT_STACKOVERFLOW'.casefold(): 1,\n            'CAT_CULTURE'.casefold(): 2,\n            'CAT_SCIENCE'.casefold(): 3,\n            'CAT_LIFE_ARTS'.casefold(): 4,\n        }\n\n        if mode == \"test\":\n            self.labels = pd.DataFrame([[-1] * 30] * len(df))\n        else:  # train or valid\n            self.labels = df.iloc[:, 11:]\n\n        self.tokenizer = BertTokenizer.from_pretrained(\n            pretrained_model_name_or_path)\n        # self.tokenizer.add_special_tokens(\n        #     {'additional_special_tokens': [self.TBSEP]})\n        self.tokenizer.add_tokens([self.TBSEP])\n\n        tokens = [token.encode('ascii', 'replace').decode()\n                  for token in tokens if token != '']\n        added_num = self.tokenizer.add_tokens(tokens)\n        if logger:\n            logger.info(f'additional_tokens : {added_num}')\n        else:\n            print(f'additional_tokens : {added_num}')\n        # change online preprocess or off line preprocess\n        self.original_df = df\n        # res = self._preprocess_texts(df)\n        # self.prep_df = df.merge(pd.DataFrame(res), on='qa_id', how='left')\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        # change online preprocess or off line preprocess\n        # idx_row = self.prep_df.iloc[idx]\n        idx_row = self.original_df.iloc[idx].copy()\n        # idx_row = self._augment(idx_row)\n        idx_row = self.__preprocess_text_row(idx_row,\n                                             t_max_len=30,\n                                             q_max_len=239,\n                                             a_max_len=239)\n                                             # t_max_len=30,\n                                             # q_max_len=239,\n                                             # a_max_len=239)\n                                             # t_max_len=100,\n                                             # q_max_len=700,\n                                             # a_max_len=700)\n        input_ids = idx_row['input_ids'].squeeze()\n        token_type_ids = idx_row['token_type_ids'].squeeze()\n        attention_mask = idx_row['attention_mask'].squeeze()\n        qa_id = idx_row['qa_id'].squeeze()\n        # cat_labels = idx_row['cat_label'].squeeze()\n        cat_labels = -1\n        position_ids = torch.arange(self.MAX_SEQUENCE_LENGTH)\n\n        labels = self.labels.iloc[idx].values\n        return qa_id, input_ids, attention_mask, \\\n            token_type_ids, cat_labels, position_ids, labels\n\n    def _trim_input(self, title, question, answer,\n                    t_max_len, q_max_len, a_max_len):\n\n        t_len = len(title)\n        q_len = len(question)\n        a_len = len(answer)\n\n        if (t_len + q_len + a_len + 4) > self.MAX_SEQUENCE_LENGTH:\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len) / 2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len) / 2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len\n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n            # if t_new_len + a_new_len + q_new_len + 4 != self.MAX_SEQUENCE_LENGTH:\n            #     raise ValueError(\"New sequence length should be %d, but is %d\"\n            #                      % (self.MAX_SEQUENCE_LENGTH,\n            #                          (t_new_len + a_new_len + q_new_len + 4)))\n            if len(title) > t_new_len:\n                title = title[:t_new_len//2] + title[-t_new_len//2:]\n            else:\n                title = title[:t_new_len]\n            if len(question) > q_new_len:\n                question = question[:q_new_len//2] + question[-q_new_len//2:]\n            else:\n                question = question[:q_new_len]\n            if len(answer) > a_new_len:\n                answer = answer[:a_new_len//2] + answer[-a_new_len//2:]\n            else:\n                answer = answer[:a_new_len]\n        return title, question, answer\n\n    def __preprocess_text_row(self, row, t_max_len, q_max_len, a_max_len):\n        qa_id = row.qa_id\n#        title = self.tokenizer.tokenize(row.question_title)\n#        body = self.tokenizer.tokenize(row.question_body)\n#        answer = self.tokenizer.tokenize(row.answer.casefold())\n        title = self.tokenizer.tokenize(row.question_title)\n        body = self.tokenizer.tokenize(row.question_body)\n        answer = self.tokenizer.tokenize(row.answer)\n#        title = row.question_title.casefold()\n#        body = row.question_body.casefold()\n#        answer = row.answer.casefold()\n#        category = row.category\n        category = ('CAT_' + row.category).casefold()\n\n        # category \u001b$B$r\u001b(B text \u001b$B$H$7$FF~$l$F$7$^$&\u001b(B !!!\n        if self.use_category:\n            title = [category] + title\n\n        title, body, answer = self._trim_input(title, body, answer,\n                                               t_max_len=t_max_len,\n                                               q_max_len=q_max_len,\n                                               a_max_len=a_max_len)\n\n        title_and_body = title + [self.TBSEP] + body\n        # title_and_body = title + f' {self.TBSEP} ' + body\n\n        encoded_texts_dict = self.tokenizer.encode_plus(\n            text=title_and_body,\n            text_pair=answer,\n            add_special_tokens=True,\n            max_length=self.MAX_SEQUENCE_LENGTH,\n            pad_to_max_length=True,\n            return_tensors='pt',\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            return_overflowing_tokens=True,\n        )\n        encoded_texts_dict['qa_id'] = qa_id\n        encoded_texts_dict['cat_label'] = self.cat_dict[category]\n        return encoded_texts_dict\n\n    def _preprocess_texts(self, df):\n        '''\n        could be multi-processed if you need speeding up\n        '''\n        res = []\n        for i, row in tqdm(list(df.iterrows())):\n            res.append(self.__preprocess_text_row(row))\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom transformers import BertModel\n\n\nclass BertModelForBinaryMultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels, pretrained_model_name_or_path=None,\n                 cat_num=0, token_size=None, MAX_SEQUENCE_LENGTH=512):\n        super(BertModelForBinaryMultiLabelClassifier, self).__init__()\n        if pretrained_model_name_or_path:\n            self.model = BertModel.from_pretrained(\n                pretrained_model_name_or_path)\n        else:\n            raise NotImplementedError\n        self.num_labels = num_labels\n        if cat_num > 0:\n            self.catembedding = nn.Embedding(cat_num, 768)\n            self.catdropout = nn.Dropout(0.2)\n            self.catactivate = nn.ReLU()\n\n            self.catembeddingOut = nn.Embedding(cat_num, cat_num // 2 + 1)\n            self.catactivateOut = nn.ReLU()\n            self.dropout = nn.Dropout(0.2)\n            self.classifier = nn.Linear(\n                self.model.pooler.dense.out_features + cat_num // 2 + 1, num_labels)\n        else:\n            self.catembedding = None\n            self.catdropout = None\n            self.catactivate = None\n            self.catembeddingOut = None\n            self.catactivateOut = None\n            self.dropout = nn.Dropout(0.2)\n            self.classifier = nn.Linear(\n                self.model.pooler.dense.out_features, num_labels)\n\n        # resize\n        if token_size:\n            self.model.resize_token_embeddings(token_size)\n\n        # define input embedding and transformers\n        self.model.embeddings.position_embeddings = self._resize_embeddings(\n            self.model.embeddings.position_embeddings, MAX_SEQUENCE_LENGTH)\n\n        # use bertmodel as decoder\n        # self.model.config.is_decoder = True\n\n        # add modules\n        # self.add_module('my_input_embeddings', self.input_embeddings)\n        # self.add_module('my_input_bert_layer', self.input_bert_layer)\n        # self.add_module('fc_output', self.classifier)\n\n    def forward(self, input_ids=None, input_cats=None, labels=None, attention_mask=None,\n                token_type_ids=None, position_ids=None, head_mask=None,\n                inputs_embeds=None, encoder_hidden_states=None,\n                encoder_attention_mask=None):\n        if self.catembedding:\n            raise NotImplementedError\n            # encoder_hidden_states = self.catembedding(input_cats)\n            # encoder_hidden_states = self.catdropout(encoder_hidden_states)\n            # encoder_hidden_states = self.catactivate(encoder_hidden_states)\n        # if input_cats or inputs_embeds or encoder_hidden_states or encoder_attention_mask:\n        #     raise NotImplementedError\n\n        # embedding_output = self.input_embeddings(\n        #     input_ids=input_ids,\n        #     position_ids=position_ids,\n        #     token_type_ids=token_type_ids,\n        #     inputs_embeds=inputs_embeds)\n        # layer_output = self.input_bert_layer(embedding_output)\n        # inputs_embeds = layer_output[0]  # fit to bertmodel\n\n        # outputs = self.model(input_ids=input_ids_2[:, :512],\n        outputs = self.model(input_ids=input_ids,\n                             # attention_mask=attention_mask[:, :512],\n                             attention_mask=attention_mask,\n                             # token_type_ids=token_type_ids[:, :512],\n                             token_type_ids=token_type_ids,\n                             position_ids=None,\n                             head_mask=None,\n                             # inputs_embeds=inputs_embeds[:, :512, :],\n                             inputs_embeds=None,\n                             # encoder_hidden_states=inputs_embeds,\n                             encoder_hidden_states=None,\n                             encoder_attention_mask=None)\n        # pooled_output = outputs[1]\n        pooled_output = torch.mean(outputs[0], dim=1)\n        if self.catembeddingOut:\n            outcat = self.catembeddingOut(input_cats)\n            outcat = self.catactivateOut(outcat)\n            pooled_output = torch.cat([pooled_output, outcat], -1)\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        # add hidden states and attention if they are here\n        outputs = (logits,) + outputs[2:]\n\n        return outputs  # logits, (hidden_states), (attentions)\n\n    def resize_token_embeddings(self, token_num):\n        self.model.resize_token_embeddings(token_num)\n\n    def freeze_unfreeze_bert(self, freeze=True, logger=None):\n        if freeze:\n            sel_log('FREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = False\n        else:\n            sel_log('UNFREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = True\n\n    def _resize_embeddings(self, old_embeddings, new_num_tokens):\n        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n        if old_num_tokens == new_num_tokens:\n            return old_embeddings\n\n        # Build new embeddings\n        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n        new_embeddings.to(old_embeddings.weight.device)\n\n        # Copy word embeddings from the previous weights\n        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n        new_embeddings.weight.data[:num_tokens_to_copy,\n                                   :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n        return new_embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, loader, tta=False):\n    model.eval()\n\n    with torch.no_grad():\n        y_preds, qa_ids = [], []\n\n        running_loss = 0\n        for (qa_id, input_ids, attention_mask,\n             token_type_ids, cat_labels, position_ids, labels) in tqdm(loader):\n            # send them to DEVICE\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            token_type_ids = token_type_ids.to(DEVICE)\n            labels = labels.to(DEVICE)\n\n            # forward\n            outputs = model(\n                input_ids=input_ids,\n                labels=labels,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n            )\n            logits = outputs[0]\n#            loss = soft_binary_cross_entropy(logits, labels)\n\n#            running_loss += loss\n\n            y_preds.append(nn.functional.sigmoid(logits))\n#            y_trues.append(labels)\n            qa_ids.append(qa_id)\n\n#        loss_mean = running_loss / len(loader)\n\n        y_preds = torch.cat(y_preds).to('cpu').numpy()\n#        y_trues = torch.cat(y_trues).to('cpu').numpy()\n        qa_ids = torch.cat(qa_ids).to('cpu').numpy()\n\n#        metric = compute_spearmanr(y_trues, y_preds)\n\n    return y_preds, qa_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OSprint('start training!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%debug\n# prediction_loop\nimport os\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import RandomSampler\n\n\nfold_prediction_dict = {}\nfor fold in range(5):\n    OSprint(f'fold -- {fold}')\n    model = BertModelForBinaryMultiLabelClassifier(30, '/kaggle/input/guchio-quest-e002', token_size=30528, )\n    \n    model.load_state_dict(best_dict[fold])\n    model = model.to(DEVICE)\n    \n    test_dataset = QUESTDataset(\n        df=test_df,\n        mode='test',\n        augment=[],\n#        pretrained_model_name_or_path='/kaggle/input/guchio-quest-e002',\n        pretrained_model_name_or_path='/kaggle/input/google-quest-e030-head-tail',\n        tokens = [\n            'CAT_TECHNOLOGY'.casefold(),\n            'CAT_STACKOVERFLOW'.casefold(),\n            'CAT_CULTURE'.casefold(),\n            'CAT_SCIENCE'.casefold(),\n            'CAT_LIFE_ARTS'.casefold(),\n        ],\n        MAX_SEQUENCE_LENGTH=512,\n    )\n    test_sampler = RandomSampler(data_source=test_dataset)\n    test_loader = DataLoader(\n            test_dataset,\n            batch_size=BATCH_SIZE,\n            sampler=test_sampler,\n            num_workers=os.cpu_count(),\n            worker_init_fn=lambda x: np.random.seed(),\n            drop_last=False,\n            pin_memory=True\n        )\n    \n    y_preds, qa_ids = test(model, test_loader)\n    \n    fold_df = pd.DataFrame(y_preds)\n    fold_df['qa_id'] = qa_ids\n    fold_prediction_dict[fold] = fold_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# {v: k for k, v in test_dataset.tokenizer.vocab.items()}[30527]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_folds_prediction_to_sub(folds_prediction_dict):\n    for i, fold in enumerate(folds_prediction_dict):\n        if i == 0:\n            prediction = folds_prediction_dict[fold].sort_values('qa_id').set_index('qa_id').values\n        else:\n            prediction += folds_prediction_dict[fold].sort_values('qa_id').set_index('qa_id').values\n#    sub_df = pd.DataFrame(base_prediction)\n#    sub_df['qa_id'] = folds_prediction_dict[fold]['qa_id'].sort_values().values\n    prediction /= len(folds_prediction_dict)\n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = convert_folds_prediction_to_sub(fold_prediction_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_prediction = prediction.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## opt"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfrom functools import partial\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport torch\nfrom scipy.stats import spearmanr\nfrom tqdm import tqdm\n\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n\n    def __init__(self):\n        self.coef_ = 0\n\n    def _spearmanr_loss(self, coef, X, y, labels):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) +\n                     [np.inf], labels=labels)\n\n        # return -np.mean(spearmanr(y, X_p).correlation)\n        return -spearmanr(y, X_p).correlation\n\n    def fit(self, X, y, initial_coef):\n        \"\"\"\n        Optimize rounding thresholds\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        labels = self.labels\n        loss_partial = partial(self._spearmanr_loss, X=X, y=y, labels=labels)\n        self.coef_ = sp.optimize.minimize(\n            loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        labels = self.labels\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) +\n                      [np.inf], labels=labels)\n        # [np.inf], labels=[0, 1, 2, 3])\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']\n\n    def set_labels(self, labels):\n        self.labels = labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/input/google-quest-e030-head-tail/optRs.pkl', 'rb') as fin:\n    optRs = pickle.load(fin)\n\nres_prediction = []\nfor i in tqdm(list(range(30))):\n    y_pred = prediction[:, i]\n    #y_pred_argmax = np.argmax(y_pred)\n    #y_pred_argmin = np.argmin(y_pred)    \n    \n    optR = optRs[i]\n    res = optR.predict(y_pred, optR.coefficients()).astype(float)\n\n    #if len(np.unique(res)) == 1:\n    #    if np.unique(res)[0] == res[y_pred_argmax]:\n    #        res[y_pred_argmin] = np.min(y_pred)\n    #    elif np.unique(res)[0] == res[y_pred_argmin]:\n    #        if np.unique(res)[0] > 0.5:\n    #            res[y_pred_argmin] = 0\n    #        else:\n    #            res[y_pred_argmax] = 1                \n    #    else:\n    #        res[y_pred_argmax] = np.max(y_pred)\n\n    res_prediction.append(res)\n\nprediction = np.asarray(res_prediction).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nfor i in range(30):\n    plt.hist(prediction[:, i])\n    plt.title(f'{i}th label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.iloc[:, 1:] = prediction\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## avoid scoring error"},{"metadata":{"trusted":true},"cell_type":"code","source":"pub_qa_id = [39, 46, 70, 132, 200, 245, 257, 267, 284, 292, 296, 312, 322, 327, 334, 340, 357, 374, 375, 387, 391, 395, 444, 482, 483, 513, 542, 579, 589, 625, 641, 683, 725, 727, 728, 740, 748, 765, 811, 830, 851, 856, 885, 905, 929, 938, 939, 962, 1082, 1091, 1101, 1119, 1153, 1226, 1230, 1238, 1247, 1249, 1266, 1282, 1297, 1331, 1359, 1398, 1423, 1477, 1502, 1544, 1567, 1654, 1676, 1700, 1701, 1727, 1764, 1794, 1795, 1807, 1812, 1816, 1833, 1847, 1868, 1877, 1885, 1934, 1959, 1983, 1990, 2005, 2018, 2027, 2042, 2066, 2070, 2075, 2094, 2128, 2163, 2180, 2203, 2230, 2244, 2257, 2277, 2278, 2286, 2303, 2335, 2374, 2387, 2395, 2455, 2465, 2474, 2487, 2493, 2534, 2569, 2573, 2580, 2592, 2607, 2621, 2655, 2666, 2669, 2670, 2676, 2691, 2748, 2763, 2774, 2789, 2793, 2795, 2797, 2806, 2844, 2868, 2895, 2912, 2922, 2931, 2968, 3034, 3062, 3087, 3107, 3173, 3207, 3229, 3290, 3336, 3378, 3399, 3437, 3461, 3463, 3502, 3504, 3524, 3526, 3527, 3532, 3543, 3544, 3560, 3592, 3671, 3682, 3696, 3720, 3787, 3854, 3871, 3876, 3881, 3901, 3941, 3943, 3949, 3961, 3962, 3967, 4006, 4039, 4057, 4070, 4099, 4139, 4157, 4160, 4176, 4182, 4183, 4211, 4213, 4250, 4263, 4269, 4281, 4284, 4285, 4286, 4346, 4387, 4388, 4401, 4416, 4441, 4497, 4546, 4547, 4564, 4569, 4575, 4598, 4607, 4663, 4679, 4743, 4751, 4778, 4792, 4835, 4881, 4901, 4954, 4973, 5001, 5003, 5019, 5035, 5050, 5057, 5076, 5091, 5095, 5141, 5152, 5253, 5367, 5373, 5403, 5417, 5422, 5499, 5503, 5530, 5541, 5542, 5660, 5663, 5697, 5753, 5783, 5790, 5835, 5847, 5862, 5878, 5891, 5904, 5907, 5914, 5936, 5947, 5958, 5972, 5979, 5998, 6016, 6042, 6058, 6079, 6087, 6111, 6126, 6132, 6159, 6204, 6212, 6226, 6258, 6271, 6280, 6285, 6301, 6319, 6325, 6331, 6332, 6336, 6346, 6378, 6379, 6420, 6445, 6481, 6494, 6495, 6502, 6560, 6580, 6583, 6621, 6643, 6646, 6688, 6694, 6715, 6723, 6737, 6744, 6745, 6766, 6770, 6774, 6821, 6832, 6838, 6856, 6888, 6889, 6955, 6957, 6964, 6988, 6994, 7012, 7018, 7036, 7064, 7072, 7114, 7116, 7123, 7150, 7165, 7176, 7194, 7201, 7216, 7247, 7254, 7272, 7278, 7281, 7293, 7302, 7326, 7404, 7410, 7438, 7477, 7481, 7485, 7519, 7520, 7525, 7531, 7544, 7546, 7589, 7595, 7614, 7640, 7654, 7672, 7711, 7727, 7739, 7758, 7766, 7815, 7838, 7852, 7869, 7878, 7899, 7935, 7939, 7970, 8021, 8032, 8045, 8070, 8089, 8115, 8143, 8146, 8191, 8197, 8206, 8212, 8242, 8245, 8250, 8258, 8271, 8273, 8339, 8350, 8355, 8376, 8395, 8412, 8427, 8437, 8464, 8496, 8516, 8517, 8551, 8564, 8591, 8621, 8626, 8629, 8672, 8684, 8685, 8690, 8738, 8755, 8756, 8760, 8771, 8773, 8778, 8823, 8834, 8842, 8846, 8875, 8916, 8921, 8932, 8934, 8938, 8973, 8987, 9001, 9006, 9018, 9033, 9065, 9140, 9141, 9174, 9213, 9225, 9228, 9237, 9240, 9256, 9259, 9263, 9298, 9324, 9350, 9391, 9400, 9439, 9454, 9476, 9478, 9497, 9545, 9567, 9569, 9590, 9597, 9623, 9640]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30):\n    pub_y_pred = raw_prediction[sub_df.qa_id.isin(pub_qa_id).values, i+1]\n    pub_y_res = sub_df.iloc[sub_df.qa_id.isin(pub_qa_id).values, i+1].values\n\n    pub_y_pred_argmax = np.argmax(pub_y_pred)\n    pub_y_pred_argmin = np.argmin(pub_y_pred)\n\n    if len(np.unique(pub_y_res)) == 1:\n        if np.unique(pub_y_res)[0] == res[pub_y_pred_argmax]:\n            pub_y_res[pub_y_pred_argmin] = np.min(pub_y_pred)\n        elif np.unique(pub_y_res)[0] == res[pub_y_pred_argmin]:\n            if np.unique(pub_y_res)[0] > 0.5:\n                pub_y_res[pub_y_pred_argmin] = 0\n            else:\n                pub_y_res[pub_y_pred_argmax] = 1                \n        else:\n            pub_y_res[pub_y_pred_argmax] = np.max(pub_y_pred)\n    sub_df.iloc[sub_df.qa_id.isin(pub_qa_id).values, i+1] = pub_y_res\n\n    # only for sub\n    if len(pri_y_pred) == 0:\n        continue\n\n    pri_y_pred_argmax = np.argmax(pri_y_pred)\n    pri_y_pred_argmin = np.argmin(pri_y_pred)    \n\n    pri_y_pred = raw_prediction[~(sub_df.qa_id.isin(pub_qa_id).values), i+1]\n    pri_y_res = sub_df.iloc[~(sub_df.qa_id.isin(pub_qa_id).values), i+1]  \n    \n    if len(np.unique(pri_y_res)) == 1:\n        if np.unique(pri_y_res)[0] == res[pri_y_pred_argmax]:\n            pri_y_res[pri_y_pred_argmin] = np.min(pri_y_pred)\n        elif np.unique(pri_y_res)[0] == res[pri_y_pred_argmin]:\n            if np.unique(pri_y_res)[0] > 0.5:\n                pri_y_res[pri_y_pred_argmin] = 0\n            else:\n                pri_y_res[pri_y_pred_argmax] = 1                \n        else:\n            pri_y_res[pri_y_pred_argmax] = np.max(pri_y_pred)\n    sub_df.iloc[sub_df.qa_id.isin(pri_qa_id).values, i+1] = pri_y_res    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}